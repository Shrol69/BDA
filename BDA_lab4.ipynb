{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c321e2",
   "metadata": {
    "id": "38c321e2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"KMeansClustering\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "file_path = \"Customer_Data.csv\"  # Change this to your file path\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Data Preprocessing (Selecting numerical features)\n",
    "df.printSchema()  # Check schema to identify numeric columns\n",
    "features = [\"purchases_frequency\", \"balance_frequency\"]  # Replace with actual numeric column names\n",
    "# Step 4: Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(df)\n",
    "\n",
    "# Step 5: Standardize the data\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(assembled_data)\n",
    "scaled_data = scaler_model.transform(assembled_data)\n",
    "\n",
    "# Step 6: Apply K-Means Clustering\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=3)  # Choose k=3, adjust as needed\n",
    "model = kmeans.fit(scaled_data)\n",
    "clusters = model.transform(scaled_data)\n",
    "\n",
    "# Step 7: Show results\n",
    "clusters.select(\"purchases_frequency\", \"balance_frequency\", \"prediction\").show(10)  # Replace with actual feature names\n",
    "\n",
    "# Step 8: Evaluate clustering performance (Inertia / Within Set Sum of Squared Errors - WSSSE)\n",
    "wssse = model.summary.trainingCost\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "clusters_pd = clusters.select(\"purchases_frequency\", \"balance_frequency\", \"prediction\").toPandas()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"purchases_frequency\", y=\"balance_frequency\", hue=\"prediction\", palette=\"viridis\", data=clusters_pd\n",
    ")\n",
    "centers = np.array(model.clusterCenters())\n",
    "\n",
    "# Scatter plot with corrected indexing\n",
    "plt.scatter(centers[:, 0], centers[:, 1], color='red', marker='X', s=200, label=\"Centroids\")\n",
    "\n",
    "plt.title(\"K-Means Clustering Results\")\n",
    "plt.xlabel(\"Feature1\")  # Change to actual feature name\n",
    "plt.ylabel(\"Feature2\")  # Change to actual feature name\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af2cb8",
   "metadata": {
    "id": "c9af2cb8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PimaRandomForestDecisionTree\").getOrCreate()\n",
    "\n",
    "# Read CSV file (assuming the first row is header and types are inferred)\n",
    "df = spark.read.csv(\"/content/pima.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "# Assume the last column is the target\n",
    "columns = df.columns\n",
    "feature_columns = columns[:-1]  # All columns except the target\n",
    "target_column = columns[-1]\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "# Rename the target column to 'label' for ML\n",
    "df = df.withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "# Assemble features into a single vector column 'features'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "# Split the dataset into training and testing sets (70/30 split)\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "##############################\n",
    "# Random Forest Classifier\n",
    "##############################\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Evaluate the model using AUC (Area Under ROC)\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
    "print(\"Random Forest AUC:\", rf_auc)\n",
    "\n",
    "# Compute precision, recall, and accuracy using MulticlassMetrics\n",
    "# MulticlassMetrics expects an RDD of (prediction, label) pairs\n",
    "rf_pred_rdd = rf_predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics_rf = MulticlassMetrics(rf_pred_rdd)\n",
    "rf_precision = metrics_rf.precision(1.0)  # Assuming the positive class is labeled 1.0\n",
    "rf_recall = metrics_rf.recall(1.0)\n",
    "rf_accuracy = metrics_rf.accuracy\n",
    "print(\"Random Forest Precision:\", rf_precision)\n",
    "print(\"Random Forest Recall:\", rf_recall)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Print feature importances from the Random Forest model\n",
    "print(\"Random Forest Feature Importances:\")\n",
    "for col, imp in zip(feature_columns, rf_model.featureImportances):\n",
    "    print(f\"{col}: {imp}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ade286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7954,
     "status": "ok",
     "timestamp": 1741590054757,
     "user": {
      "displayName": "anaconda python",
      "userId": "01341544374191849417"
     },
     "user_tz": -330
    },
    "id": "c1ade286",
    "outputId": "bfee275c-bd3b-4453-8ffa-12795b11c472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.8238573021181713\n",
      "Random Forest Precision: 0.6666666666666666\n",
      "Random Forest Recall: 0.5797101449275363\n",
      "Random Forest Accuracy: 0.7537688442211056\n",
      "Random Forest Feature Importances:\n",
      "preg: 0.07423774049802578\n",
      "plas: 0.33818285120085156\n",
      "pres: 0.048528078434346214\n",
      "skin: 0.03609356770004231\n",
      "test: 0.051652411396612724\n",
      "mass: 0.16982738365242017\n",
      "pedi: 0.0810134543900141\n",
      "age: 0.2004645127276872\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Random Forest Classifier\n",
    "##############################\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Evaluate the model using AUC (Area Under ROC)\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
    "print(\"Random Forest AUC:\", rf_auc)\n",
    "\n",
    "# Compute precision, recall, and accuracy using MulticlassMetrics\n",
    "# MulticlassMetrics expects an RDD of (prediction, label) pairs\n",
    "rf_pred_rdd = rf_predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics_rf = MulticlassMetrics(rf_pred_rdd)\n",
    "rf_precision = metrics_rf.precision(1.0)  # Assuming the positive class is labeled 1.0\n",
    "rf_recall = metrics_rf.recall(1.0)\n",
    "rf_accuracy = metrics_rf.accuracy\n",
    "print(\"Random Forest Precision:\", rf_precision)\n",
    "print(\"Random Forest Recall:\", rf_recall)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Print feature importances from the Random Forest model\n",
    "print(\"Random Forest Feature Importances:\")\n",
    "for col, imp in zip(feature_columns, rf_model.featureImportances):\n",
    "    print(f\"{col}: {imp}\")\n",
    "\n",
    "##############################\n",
    "# Decision Tree Classifier\n",
    "##############################\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "dt_predictions = dt_model.transform(test)\n",
    "\n",
    "# Evaluate the Decision Tree model using AUC\n",
    "dt_auc = evaluator_auc.evaluate(dt_predictions)\n",
    "print(\"Decision Tree AUC:\", dt_auc)\n",
    "\n",
    "# Compute precision, recall, and accuracy for Decision Tree\n",
    "dt_pred_rdd = dt_predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics_dt = MulticlassMetrics(dt_pred_rdd)\n",
    "dt_precision = metrics_dt.precision(1.0)\n",
    "dt_recall = metrics_dt.recall(1.0)\n",
    "dt_accuracy = metrics_dt.accuracy\n",
    "print(\"Decision Tree Precision:\", dt_precision)\n",
    "print(\"Decision Tree Recall:\", dt_recall)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "\n",
    "# Optionally, print the Decision Tree structure for debugging/interpretability\n",
    "print(\"Decision Tree Model Structure:\")\n",
    "print(dt_model.toDebugString)\n",
    "\n",
    "# Stop the Spark session when finished\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
