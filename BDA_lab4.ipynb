{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c321e2",
   "metadata": {
    "id": "38c321e2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CustomerKMeansClustering\").getOrCreate()\n",
    "\n",
    "# 2. Load dataset\n",
    "file_path = \"Customer_Data.csv\"  # adjust as needed\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# 3. Choose two numeric features to cluster on (e.g. BALANCE and PURCHASES)\n",
    "features = [\"BALANCE\", \"PURCHASES\"]\n",
    "\n",
    "# 4. Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"rawFeatures\")\n",
    "assembled = assembler.transform(df).na.drop()  # drop any rows with nulls\n",
    "\n",
    "# 5. Standardize the feature vectors\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"rawFeatures\", \n",
    "    outputCol=\"features\", \n",
    "    withStd=True, \n",
    "    withMean=False\n",
    ")\n",
    "scaler_model = scaler.fit(assembled)\n",
    "scaled = scaler_model.transform(assembled)\n",
    "\n",
    "# 6. Elbow Method: compute WSSSE for k = 2…10\n",
    "wssse_list = []\n",
    "print(\"=== Elbow Method (WSSSE) ===\")\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(featuresCol=\"features\", k=k, seed=42)\n",
    "    model = km.fit(scaled)\n",
    "    cost = model.summary.trainingCost\n",
    "    wssse_list.append((k, cost))\n",
    "    print(f\"k={k:2d}, WSSSE={cost:.3f}\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# 7. Manually set optimal_k based on the elbow plot\n",
    "optimal_k = 3  # ← update this after you inspect the printed WSSSE\n",
    "\n",
    "# 8. Fit final KMeans model\n",
    "km_final = KMeans(featuresCol=\"features\", k=optimal_k, seed=42)\n",
    "model_final = km_final.fit(scaled)\n",
    "clusters = model_final.transform(scaled)\n",
    "\n",
    "# 9. Show a sample of BALANCE, PURCHASES and assigned cluster\n",
    "print(\"=== Sample cluster assignments ===\")\n",
    "clusters.select(\"BALANCE\", \"PURCHASES\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# 10. Convert to pandas for plotting\n",
    "clusters_pd = clusters.select(\"BALANCE\", \"PURCHASES\", \"prediction\").toPandas()\n",
    "\n",
    "# 11. Recover original-scale centroids\n",
    "#     Note: scaler_model.std is a DenseVector of std devs\n",
    "std_vector = scaler_model.std.toArray()\n",
    "scaled_centers = np.array(model_final.clusterCenters())\n",
    "orig_centers = scaled_centers * std_vector  # inverse scaling\n",
    "\n",
    "# 12. Plot clusters with seaborn + centroids\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=clusters_pd,\n",
    "    x=\"BALANCE\", y=\"PURCHASES\",\n",
    "    hue=\"prediction\", palette=\"tab10\", s=50\n",
    ")\n",
    "plt.scatter(\n",
    "    orig_centers[:, 0], orig_centers[:, 1],\n",
    "    marker='X', s=200, color='red', label='Centroids'\n",
    ")\n",
    "plt.title(f\"K-Means Clustering on Customer Data (k={optimal_k})\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.ylabel(\"Purchases\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 13. Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af2cb8",
   "metadata": {
    "id": "c9af2cb8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PimaRandomForestDecisionTree\").getOrCreate()\n",
    "\n",
    "# Read CSV file (assuming the first row is header and types are inferred)\n",
    "df = spark.read.csv(\"/content/pima.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "# Assume the last column is the target\n",
    "columns = df.columns\n",
    "feature_columns = columns[:-1]  # All columns except the target\n",
    "target_column = columns[-1]\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "# Rename the target column to 'label' for ML\n",
    "df = df.withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "# Assemble features into a single vector column 'features'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "# Split the dataset into training and testing sets (70/30 split)\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "##############################\n",
    "# Random Forest Classifier\n",
    "##############################\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Evaluate the model using AUC (Area Under ROC)\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
    "print(\"Random Forest AUC:\", rf_auc)\n",
    "\n",
    "# Compute precision, recall, and accuracy using MulticlassMetrics\n",
    "# MulticlassMetrics expects an RDD of (prediction, label) pairs\n",
    "rf_pred_rdd = rf_predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics_rf = MulticlassMetrics(rf_pred_rdd)\n",
    "rf_precision = metrics_rf.precision(1.0)  # Assuming the positive class is labeled 1.0\n",
    "rf_recall = metrics_rf.recall(1.0)\n",
    "rf_accuracy = metrics_rf.accuracy\n",
    "print(\"Random Forest Precision:\", rf_precision)\n",
    "print(\"Random Forest Recall:\", rf_recall)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Print feature importances from the Random Forest model\n",
    "print(\"Random Forest Feature Importances:\")\n",
    "for col, imp in zip(feature_columns, rf_model.featureImportances):\n",
    "    print(f\"{col}: {imp}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ade286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7954,
     "status": "ok",
     "timestamp": 1741590054757,
     "user": {
      "displayName": "anaconda python",
      "userId": "01341544374191849417"
     },
     "user_tz": -330
    },
    "id": "c1ade286",
    "outputId": "bfee275c-bd3b-4453-8ffa-12795b11c472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.8238573021181713\n",
      "Random Forest Precision: 0.6666666666666666\n",
      "Random Forest Recall: 0.5797101449275363\n",
      "Random Forest Accuracy: 0.7537688442211056\n",
      "Random Forest Feature Importances:\n",
      "preg: 0.07423774049802578\n",
      "plas: 0.33818285120085156\n",
      "pres: 0.048528078434346214\n",
      "skin: 0.03609356770004231\n",
      "test: 0.051652411396612724\n",
      "mass: 0.16982738365242017\n",
      "pedi: 0.0810134543900141\n",
      "age: 0.2004645127276872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################\n",
    "# Decision Tree Classifier\n",
    "##############################\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "dt_predictions = dt_model.transform(test)\n",
    "\n",
    "# Evaluate the Decision Tree model using AUC\n",
    "dt_auc = evaluator_auc.evaluate(dt_predictions)\n",
    "print(\"Decision Tree AUC:\", dt_auc)\n",
    "\n",
    "# Compute precision, recall, and accuracy for Decision Tree\n",
    "dt_pred_rdd = dt_predictions.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics_dt = MulticlassMetrics(dt_pred_rdd)\n",
    "dt_precision = metrics_dt.precision(1.0)\n",
    "dt_recall = metrics_dt.recall(1.0)\n",
    "dt_accuracy = metrics_dt.accuracy\n",
    "print(\"Decision Tree Precision:\", dt_precision)\n",
    "print(\"Decision Tree Recall:\", dt_recall)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "\n",
    "# Optionally, print the Decision Tree structure for debugging/interpretability\n",
    "print(\"Decision Tree Model Structure:\")\n",
    "print(dt_model.toDebugString)\n",
    "\n",
    "# Stop the Spark session when finished\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
