{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Ws2tx4Djqxaw",
        "outputId": "eba81296-770f-4a2b-d0d0-d1032db64246"
      },
      "outputs": [],
      "source": [
        "# Untar the Spark installer\n",
        "!tar -xvf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "# Checking the Spark folder after untar\n",
        "!ls \n",
        "# Installing findspark - a python library to find Spark\n",
        "!pip install -q findspark\n",
        "# Setting environment variables: Setting Java and Spark home based on the location where they are stored\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "# Creating a local Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read CSV file with error handling\n",
        "# Read CSV file with error handling for newer pandas versions\n",
        "amazon_df = pd.read_csv(\"/content/Amazon_Responded_Oct05.csv\", on_bad_lines='skip')  \n",
        "# or for older versions\n",
        "# amazon_df = pd.read_csv(\"/content/Amazon_Responded_Oct05.csv\")\n",
        "amazon_df.head()\n",
        "amazon_df.shape\n",
        "# Dropping all null rows\n",
        "amazon_df.dropna(how=\"all\", inplace=True)\n",
        "amazon_df.shape\n",
        "# Replacing carriage return and new line characters with a space\n",
        "amazon_df = amazon_df.replace({r'\\r\\n': ' '}, regex=True)\n",
        "amazon_df.head()\n",
        "# Converting Pandas Dataframe into Spark Dataframe\n",
        "amazon_df = amazon_df.astype(str) # Converting pandas df to string first\n",
        "amazon_sdf = spark.createDataFrame(amazon_df)\n",
        "amazon_sdf.show(10, False) # False allows us to show entire content of the columns\n",
        "# Columns in df\n",
        "amazon_sdf.columns\n",
        "# Schema: Datatypes associated with columns\n",
        "amazon_sdf.printSchema()\n",
        "# Total number of rows\n",
        "amazon_sdf.count()\n",
        "# Extracting columns 'user_id_str', 'user_followers_count', and 'text_'\n",
        "amazon_sub_df = amazon_sdf.select(amazon_sdf.user_id_str, amazon_sdf.user_followers_count.cast('int').alias('user_followers_count'), amazon_sdf.text_)\n",
        "amazon_sub_df.show(20, False)\n",
        "# Checking datatype of columns\n",
        "amazon_sub_df.printSchema()\n",
        "# Number of distinct users \n",
        "import pyspark.sql.functions as f\n",
        "amazon_sub_df.select(f.countDistinct(\"user_id_str\")).show()\n",
        "# Checking number of rows (tweets) for a particular user\n",
        "amazon_sub_df.filter(amazon_sub_df.user_id_str == '85741735.0').count()\n",
        "# Removing duplicate records by keeping just the maximum number of followers for any user\n",
        "# Step 1: First let us get the max followers for every user\n",
        "import pyspark.sql.functions as f\n",
        "maxf = amazon_sub_df.groupBy(\"user_id_str\").agg(f.max(\"user_followers_count\").alias(\"max\")).alias(\"maxf\")\n",
        "maxf.show()\n",
        "# Total number of rows\n",
        "maxf.count()\n",
        "# Step 2: Let us now join this with the original df (amazon_sub_df) to get all the rows of users which match their max follower count obtained in 'maxf'\n",
        "from pyspark.sql.functions import col \n",
        "amazon_sub_df = amazon_sub_df.alias(\"amazon_sub_df\") # defining alias for original df\n",
        "amazon_sub_df2 = amazon_sub_df.join(maxf, (col(\"user_followers_count\") == col(\"max\")) & \n",
        "                                    (col(\"amazon_sub_df.user_id_str\") == col(\"maxf.user_id_str\"))).select(\n",
        "                                     col(\"amazon_sub_df.user_id_str\"), col(\"amazon_sub_df.user_followers_count\"), col(\"amazon_sub_df.text_\"))\n",
        "amazon_sub_df2.show(20, False)\n",
        "# Total number of rows\n",
        "amazon_sub_df2.count()\n",
        "# Checking number of rows (tweets) for that user again\n",
        "amazon_sub_df2.filter(amazon_sub_df2.user_id_str == '85741735.0').count()\n",
        "# Creating a filter to find popular users who have more than 5000 followers\n",
        "popular_df = amazon_sub_df2.filter(amazon_sub_df2.user_followers_count >= 5000)\n",
        "popular_df.count() # number of rows\n",
        "popular_df.show(20, False)\n",
        "# Check: Sorting follower count Ascending\n",
        "popular_df.sort(popular_df.user_followers_count).show(20, False)\n",
        "# Check: Sorting follower count Descending\n",
        "popular_df.sort((popular_df.user_followers_count).desc()).show(20, False)\n",
        "# Number of distinct users\n",
        "popular_df.select('user_id_str').distinct().count()\n",
        "# Finding number of tweets per user using groupBy\n",
        "groupedUsers = popular_df.groupby('user_id_str').count().withColumnRenamed(\"count\",\"tweet_count\")\n",
        "# Sorting in descending order of count\n",
        "groupedUsers.sort((groupedUsers.tweet_count).desc()).show(20)\n",
        "# Counting words frequency of the tweets posted by the popular users from step 4\n",
        "# Reading column 'text_' from spark df (popular_df) to a python list, which will then be read into RDD object \n",
        "tweet = popular_df.select(\"text_\").rdd.flatMap(lambda x: x).collect()\n",
        "tweet[0:5]\n",
        "# Creating Spark Context\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "# Reading 'tweet' to a RDD object using spark context\n",
        "tweet_rdd = sc.parallelize(tweet)\n",
        "tweet_rdd.take(5)\n",
        "# Function for cleaning the text of tweets\n",
        "import string\n",
        "import re\n",
        "\n",
        "def clean_tweet(x):\n",
        "  \n",
        "  # Delete all the URLs in the tweets\n",
        "  text00 = re.sub(r'www\\S+', '', x)\n",
        "  text01 = re.sub(r'http\\S+', '', text00)\n",
        "  \n",
        "  # Delete all the numbers in the tweets\n",
        "  text1 = ''.join([i for i in text01 if not i.isdigit()])\n",
        "  \n",
        "  # Delete all the punctuation marks in the tweets\n",
        "  text2 = text1.translate(str.maketrans('','',string.punctuation))\n",
        "  \n",
        "  # Convert text to LOWERCASE\n",
        "  text3 = text2.lower()\n",
        "\n",
        "  return text3\n",
        "\n",
        "# Cleaning the text of tweets: 1. Removing URLs, 2. Removing non-alphabets, 3. Lowercase \n",
        "clean_tweet_rdd = tweet_rdd.map(clean_tweet)\n",
        "clean_tweet_rdd.take(10)\n",
        "# Building Map function\n",
        "map = clean_tweet_rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
        "map.take(5)\n",
        "# Building Reduce function\n",
        "counts = map.reduceByKey(lambda a, b: a + b)\n",
        "counts.take(5)\n",
        "# Total number of distinct words\n",
        "print(len(counts.collect()))\n",
        "\n",
        "# Getting the Top 10 most popular words and their words frequency\n",
        "# Sorting 'counts' in descending order and getting the first 10 elements\n",
        "countsSortedTopTen = counts.takeOrdered(10, lambda a: -a[1] if len(a[0]) > 0 else False) # Conditioned on that number of characters in the string should be at least 1\n",
        "countsSortedTopTen"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
